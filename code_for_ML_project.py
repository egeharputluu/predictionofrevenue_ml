# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PbiVdE_wiML3WqmuyhpXJjA2UacM9zv4
"""

#Step 1: Load the Data and Required Libraries !!! !!! !!!

#Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from google.colab import files


#Load the dataset
print("Please upload a file with CSV format:")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
dataset = pd.read_csv(file_name)

#Display the first 10 rows of the dataset to understand its structure
print("First 10 rows of the original dataset:")
print(dataset.head(10))

print("\nGeneral Info about the dataset:")
print(dataset.info())



#Step 2: Data Preprocessing !!! !!! !!!

#Remove Unnecessary Features
#Drop the 'University_Count' feature as it overlaps with other features like 'Daily_Foot_Traffic' ('University_Count' was removed because its effect on revenue is already represented by 'Daily_Foot_Traffic'.)
dataset = dataset.drop(columns=["University_Count"])

#Handle missing data if exists
print("Checking for missing values:")
print(dataset.isnull().sum())

#Fill numeric columns with the mean
numeric_columns = dataset.select_dtypes(include=["float64", "int64"]).columns
dataset[numeric_columns] = dataset[numeric_columns].fillna(dataset[numeric_columns].mean())

#Fill categorical columns with the mode because for categoric values, mod matters!
categorical_columns = dataset.select_dtypes(include=["object"]).columns
for col in categorical_columns:
    dataset[col] = dataset[col].fillna(dataset[col].mode()[0])


print("\nMissing values after handling:")
print(dataset.isnull().sum())

#One-hot encoding for the Location column becase it has categoric values and needs to be numerical to impelement coding! Here, if 2 of 3 locations encoded as 0, this means other one encoded as 1!
dataset = pd.get_dummies(dataset, columns=["Location"], drop_first=True)


#Split Features and Target
#Split the dataset into features (X) and target variable (y)
X = dataset.drop(columns=["Estimated_Revenue"])  #Features
y = dataset["Estimated_Revenue"]  #Target

X_original = X.copy() #Maybe it will be needed to compare with original ones so I copied X


#Normalize the features using Min-Max Scaling (scales features to a range of [0, 1])
numeric_columns = X.select_dtypes(include=["float64", "int64"]).columns
categorical_columns = X.select_dtypes(include=["uint8"]).columns

#Apply Min-Max Scaling only to numerical columns because one-hot encoded feature (location) is already normalized automatically and defaultly!
scaler = MinMaxScaler()
X_normalized_numeric = pd.DataFrame(scaler.fit_transform(X[numeric_columns]), columns=numeric_columns)

#Combine normalized numerical columns and untouched categorical columns
X_normalized = pd.concat([X_normalized_numeric, X[categorical_columns].reset_index(drop=True)], axis=1)

#Display the first few rows of the normalized dataset
print("\nFirst 10 rows of the normalized dataset:")
print(X_normalized.head(10))



#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# Coreallation analysis and remove the low corellations
#correlation_matrix = X_normalized.corr()
#target_correlation = correlation_matrix.iloc[:, -1]  #Target is the last column!
#ow_corr_features = target_correlation[abs(target_correlation) < 0.1].index
#rint("Column with low-corelations with the target variable:")
#print(low_corr_features)

#Remove the low-corealitons
#X_normalized = X_normalized.drop(columns=low_corr_features)
#print("\nRemoved low-corelations. Updated normalized data-set:")
#print(X_normalized.head(10))
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!



#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#Outlier Analysis and Removal by IQR Method
def remove_outliers_iqr(df, columns):

    for col in columns:
        Q1 = df[col].quantile(0.25)  #First quartile
        Q3 = df[col].quantile(0.75)  #Third quartile
        IQR = Q3 - Q1                #Interquartile Range

        #Define outlier boundaries
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        #Filter the data to remove outliers
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

#Identify numeric columns for outlier analysis
numeric_columns = X.select_dtypes(include=["float64", "int64"]).columns

#Remove outliers
print(f"\nDataset shape before removing outliers: {dataset.shape}")
dataset = remove_outliers_iqr(dataset, numeric_columns)
print(f"Dataset shape after removing outliers: {dataset.shape}")

#Display the first 10 rows of the cleaned from outliers dataset
print("\nFirst 10 rows of the dataset after removing outliers:")
print(dataset.head(10))
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


#Split the normalized data into training (70%) and testing (30%) sets
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.3, random_state=42)

#Display the shape of the training and testing sets
print(f"\nTraining Set Shape: {X_train.shape}")
print(f"Testing Set Shape: {X_test.shape}")



#Step 3: Implementing Linear Regression !!! !!! !!!

#Initialize and train the Linear Regression model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

#Make predictions on the test set
y_pred_lin = lin_reg.predict(X_test)

#Evaluate the Linear Regression model
mae_lin = mean_absolute_error(y_test, y_pred_lin)
mse_lin = mean_squared_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)

#Display performance metrics
print("\nLinear Regression Performance:")
print(f"Mean Absolute Error (MAE): {mae_lin:.2f}")
print(f"Mean Squared Error (MSE): {mse_lin:.2f}")
print(f"Root Mean Squared Error (RMSE): {np.sqrt(mse_lin):.2f}")
print(f"R² Score: {r2_lin:.2f}")



#Step 4: Implementing Random Forest for Regression !!! !!! !!!

#Initialize and train the Random Forest Regressor
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train, y_train)

#Make predictions on the test set
y_pred_rf = rf_reg.predict(X_test)

#Evaluate the Random Forest Regressor model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

#Display performance metrics
print("\nRandom Forest Regressor Performance:")
print(f"Mean Absolute Error (MAE): {mae_rf:.2f}")
print(f"Mean Squared Error (MSE): {mse_rf:.2f}")
print(f"Root Mean Squared Error (RMSE): {np.sqrt(mse_rf):.2f}")
print(f"R² Score: {r2_rf:.2f}")





#Step 5: Compare and Interpret Results !!! !!! !!!

#Compare these two models
print("\nModel Comparison:")
print(f"Linear Regression R² Score: {r2_lin:.2f}")
print(f"Random Forest R² Score: {r2_rf:.2f}")


print("\n****************************************************************************************************\n")

results = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest Regressor"],
    "MAE": [mae_lin, mae_rf],
    "MSE": [mse_lin, mse_rf],
    "RMSE": [np.sqrt(mse_lin), np.sqrt(mse_rf)],
    "R² Score": [r2_lin, r2_rf]
})

print("\nModel Performance Comparison:\n")
print(results)



#Random Forest Algorithm is expected to perform better than Linear Regression for this task with the data-set !!!
#The comparison helps identify the most suitable model for this task.